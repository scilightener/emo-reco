{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM network for emotion detection in twits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- based on: https://aclanthology.org/S19-2034/\n",
    "- code taken from here: https://github.com/sismetanin/emosense-semeval2019-task3-emocontext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import io\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "label2emotion = {0: \"others\", 1: \"happy\", 2: \"sad\", 3: \"angry\"}\n",
    "emotion2label = {\"others\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3}\n",
    "\n",
    "emoticons_additional = {\n",
    "    '(^・^)': '<happy>', ':‑c': '<sad>', '=‑d': '<happy>', \":'‑)\": '<happy>', ':‑d': '<laugh>',\n",
    "    ':‑(': '<sad>', ';‑)': '<happy>', ':‑)': '<happy>', ':\\\\/': '<sad>', 'd=<': '<annoyed>',\n",
    "    ':‑/': '<annoyed>', ';‑]': '<happy>', '(^�^)': '<happy>', 'angru': 'angry', \"d‑':\":\n",
    "        '<annoyed>', \":'‑(\": '<sad>', \":‑[\": '<annoyed>', '(�?�)': '<happy>', 'x‑d': '<laugh>',\n",
    "}\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "               'time', 'url', 'date', 'number'],\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "              'emphasis', 'censored'},\n",
    "    fix_html=True,\n",
    "    segmenter=\"twitter\",\n",
    "    corrector=\"twitter\",\n",
    "    unpack_hashtags=True,\n",
    "    unpack_contractions=True,\n",
    "    spell_correct_elong=True,\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    dicts=[emoticons, emoticons_additional]\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \" \".join(text_processor.pre_process_doc(text))\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocessData(dataFilePath, mode):\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            line = line.strip().split('\\t')\n",
    "            for i in range(1, 4):\n",
    "                line[i] = tokenize(line[i])\n",
    "            if mode == \"train\":\n",
    "                labels.append(emotion2label[line[4]])\n",
    "            conv = line[1:4]\n",
    "            conversations.append(conv)\n",
    "    if mode == \"train\":\n",
    "        return np.array(conversations), np.array(labels)\n",
    "    else:\n",
    "        return np.array(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, labels_train = preprocessData('data\\\\train.txt', mode=\"train\")\n",
    "texts_dev, labels_dev = preprocessData('data\\\\dev.txt', mode=\"train\")\n",
    "texts_test, labels_test = preprocessData('data\\\\test.txt', mode=\"train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddings(file):\n",
    "    embeddingsIndex = {}\n",
    "    dim = 0\n",
    "    with io.open(file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingsIndex[word] = embeddingVector \n",
    "            dim = len(embeddingVector)\n",
    "    return embeddingsIndex, dim\n",
    "\n",
    "\n",
    "def getEmbeddingMatrix(wordIndex, embeddings, dim):\n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, dim))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingMatrix[i] = embeddings.get(word)\n",
    "    return embeddingMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 658129 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "embeddings, dim = getEmbeddings('emosense.300d.txt')\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts([' '.join(list(embeddings.keys()))])\n",
    "\n",
    "wordIndex = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(wordIndex))\n",
    "\n",
    "embeddings_matrix = getEmbeddingMatrix(wordIndex, embeddings, dim) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Texts Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 24\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(texts_train, labels_train, test_size=0.2, random_state=42)\n",
    "\n",
    "labels_categorical_train = to_categorical(np.asarray(y_train))\n",
    "labels_categorical_val = to_categorical(np.asarray(y_val))\n",
    "labels_categorical_dev = to_categorical(np.asarray(labels_dev))\n",
    "labels_categorical_test = to_categorical(np.asarray(labels_test))\n",
    "\n",
    "\n",
    "def get_sequences(texts, sequence_length):\n",
    "    message_first = pad_sequences(tokenizer.texts_to_sequences(texts[:, 0]), sequence_length)\n",
    "    message_second = pad_sequences(tokenizer.texts_to_sequences(texts[:, 1]), sequence_length)\n",
    "    message_third = pad_sequences(tokenizer.texts_to_sequences(texts[:, 2]), sequence_length)\n",
    "    return message_first, message_second, message_third\n",
    "\n",
    "\n",
    "message_first_message_train, message_second_message_train, message_third_message_train = get_sequences(X_train, MAX_SEQUENCE_LENGTH)\n",
    "message_first_message_val, message_second_message_val, message_third_message_val = get_sequences(X_val, MAX_SEQUENCE_LENGTH)\n",
    "message_first_message_dev, message_second_message_dev, message_third_message_dev = get_sequences(texts_dev, MAX_SEQUENCE_LENGTH)\n",
    "message_first_message_test, message_second_message_test, message_third_message_test = get_sequences(texts_test, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bidirectional LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Concatenate, \\\n",
    "    Dropout, LSTM, Bidirectional, GaussianNoise\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def buildModel(embeddings_matrix, sequence_length, lstm_dim, hidden_layer_dim, num_classes, \n",
    "               noise=0.1, dropout_lstm=0.2, dropout=0.2):\n",
    "    turn1_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    turn2_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    turn3_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "    embedding_dim = embeddings_matrix.shape[1]\n",
    "    embeddingLayer = Embedding(embeddings_matrix.shape[0],\n",
    "                                embedding_dim,\n",
    "                                weights=[embeddings_matrix],\n",
    "                                input_length=sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    turn1_branch = embeddingLayer(turn1_input)\n",
    "    turn2_branch = embeddingLayer(turn2_input) \n",
    "    turn3_branch = embeddingLayer(turn3_input) \n",
    "    \n",
    "    turn1_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn1_branch)\n",
    "    turn2_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn2_branch)\n",
    "    turn3_branch = GaussianNoise(noise, input_shape=(None, sequence_length, embedding_dim))(turn3_branch)\n",
    "\n",
    "    lstm1 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
    "    lstm2 = Bidirectional(LSTM(lstm_dim, dropout=dropout_lstm))\n",
    "    \n",
    "    turn1_branch = lstm1(turn1_branch)\n",
    "    turn2_branch = lstm2(turn2_branch)\n",
    "    turn3_branch = lstm1(turn3_branch)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([turn1_branch, turn2_branch, turn3_branch])\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Dense(hidden_layer_dim, activation='relu')(x)\n",
    "    \n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[turn1_input, turn2_input, turn3_input], outputs=output)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = buildModel(embeddings_matrix, MAX_SEQUENCE_LENGTH, lstm_dim=64, hidden_layer_dim=30, num_classes=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 24)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 24)]         0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 24)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 24, 300)      197439000   ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]',                \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " gaussian_noise (GaussianNoise)  (None, 24, 300)     0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " gaussian_noise_1 (GaussianNois  (None, 24, 300)     0           ['embedding[1][0]']              \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " gaussian_noise_2 (GaussianNois  (None, 24, 300)     0           ['embedding[2][0]']              \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 128)          186880      ['gaussian_noise[0][0]',         \n",
      "                                                                  'gaussian_noise_2[0][0]']       \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 128)         186880      ['gaussian_noise_1[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 384)          0           ['bidirectional[0][0]',          \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'bidirectional[1][0]']          \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 384)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 30)           11550       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4)            124         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 197,824,434\n",
      "Trainable params: 385,434\n",
      "Non-trainable params: 197,439,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kutilities.callbacks import MetricsCallback\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "metrics = {\n",
    "    \"f1_e\": (lambda y_test, y_pred:\n",
    "             f1_score(y_test, y_pred, average='micro',\n",
    "                      labels=[emotion2label['happy'],\n",
    "                              emotion2label['sad'],\n",
    "                              emotion2label['angry']\n",
    "                              ])),\n",
    "    \"precision_e\": (lambda y_test, y_pred:\n",
    "                    precision_score(y_test, y_pred, average='micro',\n",
    "                                    labels=[emotion2label['happy'],\n",
    "                                            emotion2label['sad'],\n",
    "                                            emotion2label['angry']\n",
    "                                            ])),\n",
    "    \"recall_e\": (lambda y_test, y_pred:\n",
    "                 recall_score(y_test, y_pred, average='micro',\n",
    "                              labels=[emotion2label['happy'],\n",
    "                                      emotion2label['sad'],\n",
    "                                      emotion2label['angry']\n",
    "                                      ]))\n",
    "}\n",
    "\n",
    "filepath = \"models\\\\bidirectional_LSTM_best_weights_{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc', save_best_only=True, save_weights_only=False, mode='auto')\n",
    "tensorboardCallback = TensorBoard(log_dir='Graph', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "121/121 [==============================] - 43s 276ms/step - loss: 0.8040 - acc: 0.6696 - val_loss: 0.4088 - val_acc: 0.8442\n",
      "Epoch 2/20\n",
      "121/121 [==============================] - 32s 265ms/step - loss: 0.4296 - acc: 0.8396 - val_loss: 0.3245 - val_acc: 0.8876\n",
      "Epoch 3/20\n",
      "121/121 [==============================] - 32s 266ms/step - loss: 0.3649 - acc: 0.8658 - val_loss: 0.3008 - val_acc: 0.8922\n",
      "Epoch 4/20\n",
      "121/121 [==============================] - 32s 266ms/step - loss: 0.3249 - acc: 0.8818 - val_loss: 0.2823 - val_acc: 0.9020\n",
      "Epoch 5/20\n",
      "121/121 [==============================] - 31s 255ms/step - loss: 0.3014 - acc: 0.8907 - val_loss: 0.2649 - val_acc: 0.9096\n",
      "Epoch 6/20\n",
      "121/121 [==============================] - 29s 243ms/step - loss: 0.2767 - acc: 0.9006 - val_loss: 0.2599 - val_acc: 0.9072\n",
      "Epoch 7/20\n",
      "121/121 [==============================] - 29s 240ms/step - loss: 0.2613 - acc: 0.9059 - val_loss: 0.2491 - val_acc: 0.9090\n",
      "Epoch 8/20\n",
      "121/121 [==============================] - 31s 254ms/step - loss: 0.2494 - acc: 0.9099 - val_loss: 0.2416 - val_acc: 0.9145\n",
      "Epoch 9/20\n",
      "121/121 [==============================] - 29s 242ms/step - loss: 0.2317 - acc: 0.9172 - val_loss: 0.2405 - val_acc: 0.9138\n",
      "Epoch 10/20\n",
      "121/121 [==============================] - 31s 254ms/step - loss: 0.2228 - acc: 0.9193 - val_loss: 0.2459 - val_acc: 0.9153\n",
      "Epoch 11/20\n",
      "121/121 [==============================] - 29s 244ms/step - loss: 0.2108 - acc: 0.9230 - val_loss: 0.2358 - val_acc: 0.9146\n",
      "Epoch 12/20\n",
      "121/121 [==============================] - 32s 261ms/step - loss: 0.1954 - acc: 0.9294 - val_loss: 0.2351 - val_acc: 0.9176\n",
      "Epoch 13/20\n",
      "121/121 [==============================] - 31s 258ms/step - loss: 0.1887 - acc: 0.9315 - val_loss: 0.2345 - val_acc: 0.9188\n",
      "Epoch 14/20\n",
      "121/121 [==============================] - 30s 246ms/step - loss: 0.1758 - acc: 0.9369 - val_loss: 0.2411 - val_acc: 0.9173\n",
      "Epoch 15/20\n",
      "121/121 [==============================] - 30s 244ms/step - loss: 0.1706 - acc: 0.9388 - val_loss: 0.2427 - val_acc: 0.9156\n",
      "Epoch 16/20\n",
      "121/121 [==============================] - 31s 258ms/step - loss: 0.1638 - acc: 0.9414 - val_loss: 0.2418 - val_acc: 0.9199\n",
      "Epoch 17/20\n",
      "121/121 [==============================] - 29s 242ms/step - loss: 0.1535 - acc: 0.9447 - val_loss: 0.2454 - val_acc: 0.9181\n",
      "Epoch 18/20\n",
      "121/121 [==============================] - 29s 244ms/step - loss: 0.1478 - acc: 0.9467 - val_loss: 0.2543 - val_acc: 0.9181\n",
      "Epoch 19/20\n",
      "121/121 [==============================] - 29s 243ms/step - loss: 0.1360 - acc: 0.9518 - val_loss: 0.2535 - val_acc: 0.9174\n",
      "Epoch 20/20\n",
      "121/121 [==============================] - 29s 242ms/step - loss: 0.1314 - acc: 0.9521 - val_loss: 0.2579 - val_acc: 0.9193\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([message_first_message_train, message_second_message_train, message_third_message_train],\n",
    "                    np.array(labels_categorical_train),\n",
    "                    callbacks=[checkpoint, tensorboardCallback],\n",
    "                    validation_data=(\n",
    "                        [message_first_message_val, message_second_message_val, message_third_message_val],\n",
    "                        np.array(labels_categorical_val)\n",
    "                    ),\n",
    "                    epochs=20,\n",
    "                    batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 725.17 seconds\n"
     ]
    }
   ],
   "source": [
    "print('training time: {:.2f} seconds'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"models\\\\bidirectional_LSTM_best_weights_16-0.9199.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 3s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([message_first_message_dev, message_second_message_dev, message_third_message_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_e 0.7052845528455286\n",
      "precision_e 0.6119929453262787\n",
      "recall_e 0.8321342925659473\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94      2338\n",
      "           1       0.55      0.79      0.65       142\n",
      "           2       0.72      0.82      0.77       125\n",
      "           3       0.61      0.88      0.72       150\n",
      "\n",
      "    accuracy                           0.90      2755\n",
      "   macro avg       0.71      0.85      0.77      2755\n",
      "weighted avg       0.92      0.90      0.91      2755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for title, metric in metrics.items():\n",
    "    print(title, metric(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "print(classification_report(labels_categorical_dev.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/173 [==============================] - 5s 31ms/step\n",
      "prediction time: 5.52 seconds\n",
      "f1_e 0.7099433281813499\n",
      "precision_e 0.6212804328223624\n",
      "recall_e 0.828125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94      4677\n",
      "           1       0.59      0.80      0.68       284\n",
      "           2       0.68      0.84      0.75       250\n",
      "           3       0.61      0.85      0.71       298\n",
      "\n",
      "    accuracy                           0.90      5509\n",
      "   macro avg       0.71      0.85      0.77      5509\n",
      "weighted avg       0.92      0.90      0.91      5509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "y_pred = model.predict([message_first_message_test, message_second_message_test, message_third_message_test])\n",
    "\n",
    "print('prediction time: {:.2f} seconds'.format(time.time() - start))\n",
    "\n",
    "for title, metric in metrics.items():\n",
    "    print(title, metric(labels_categorical_test.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "print(classification_report(labels_categorical_test.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
